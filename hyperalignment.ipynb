{
  "metadata": {
    "name": "Hyperalignment for between-subject analysis"
  }, 
  "nbformat": 3, 
  "nbformat_minor": 0, 
  "worksheets": [
    {
      "cells": [
        {
          "cell_type": "heading", 
          "level": 1, 
          "metadata": {}, 
          "source": [
            "Hyperalignment for between-subject analysis"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "Multivariate pattern analysis (MVPA) reveals how the brain represents\nfine-scale information. Its power lies in its sensitivity to subtle pattern\nvariations that encode this fine-scale information but that also presents a\nhurdle for group analyses due to between-subject variability of both anatomical\n& functional architectures. ", 
            "*Haxby et al. (2011)* recently\nproposed a method of aligning subjects' brain data in a high-dimensional\nfunctional space and showed how to build a common model of ventral temporal\ncortex that captures visual object category information. They tested their\nmodel by successfully performing between-subject classification of category\ninformation.  Moreover, when they built the model using a complex naturalistic\nstimulation (a feature film), it even generalized to other independent\nexperiments even after removing any occurrences of the experimental stimuli\nfrom the movie data.\n\n", 
            "In this example we show how to perform Hyperalignment within a single\nexperiment. We will compare between-subject classification after hyperalignment\nto between-subject classification on anatomically aligned data (currently the\nmost typical approach), and within-subject classification performance."
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Analysis setup"
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "from mvpa2.suite import *\n\nverbose.level = 2"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "We start by loading preprocessed datasets of 10 subjects with BOLD-responses\nof stimulation with face and object images (", 
            "*Haxby et al., 2011*).\nEach dataset, after preprocessing, has one sample per category and run for each\nof the eight runs and seven stimulus categories. Individual subject brains have\nbeen aligned anatomically using a 12 dof linear transformation."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "verbose(1, \"Loading data...\")\nfilepath = os.path.join(cfg.get('location', 'tutorial data'),\n                        'hyperalignment_tutorial_data.hdf5.gz')\nds_all = h5load(filepath)\n# zscore all datasets individually\n_ = [zscore(ds) for ds in ds_all]\n# inject the subject ID into all datasets\nfor i,sd in enumerate(ds_all):\n    sd.sa['subject'] = np.repeat(i, len(sd))\n# number of subjects\nnsubjs = len(ds_all)\n# number of categories\nncats = len(ds_all[0].UT)\n# number of run\nnruns = len(ds_all[0].UC)\nverbose(2, \"%d subjects\" % len(ds_all))\nverbose(2, \"Per-subject dataset: %i samples with %i features\" % ds_all[0].shape)\nverbose(2, \"Stimulus categories: %s\" % ', '.join(ds_all[0].UT))"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "Now we'll create a couple of building blocks for the intended analyses. We'll\nuse a linear SVM classifier, and perform feature selection with a simple one-way\nANOVA selecting the `nf` highest scoring features."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "# use same classifier\nclf = LinearCSVMC()\n\n# feature selection helpers\nnf = 100\nfselector = FixedNElementTailSelector(nf, tail='upper',\n                                      mode='select',sort=False)\nsbfs = SensitivityBasedFeatureSelection(OneWayAnova(), fselector,\n                                        enable_ca=['sensitivities'])\n# create classifier with automatic feature selection\nfsclf = FeatureSelectionClassifier(clf, sbfs)"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Within-subject classification"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "We start off by running a cross-validated classification analysis for every\nsubject's dataset individually. Data folding will be performed by leaving out\none run to serve as the testing dataset. ANOVA-based features selection will be\nperformed automatically on training dataset and applied to testing dataset."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "verbose(1, \"Performing classification analyses...\")\nverbose(2, \"within-subject...\", cr=False, lf=False)\nwsc_start_time = time.time()\ncv = CrossValidation(fsclf,\n                     NFoldPartitioner(attr='chunks'),\n                     errorfx=mean_match_accuracy)\n# store results in a sequence\nwsc_results = [cv(sd) for sd in ds_all]\nwsc_results = vstack(wsc_results)\nverbose(2, \" done in %.1f seconds\" % (time.time() - wsc_start_time,))"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Between-subject classification using anatomically aligned data"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "For between-subject classification with MNI-aligned voxels, we can stack up\nall individual datasets into a single one, as (anatomical!) feature\ncorrespondence is given. The crossvalidation analysis using the feature\nselection classifier will automatically perform the desired ANOVA-based feature\nselection on every training dataset partition. However, data folding will now\nbe done by leaving out a complete subject for testing."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "verbose(2, \"between-subject (anatomically aligned)...\", cr=False, lf=False)\nds_mni = vstack(ds_all)\nmni_start_time = time.time()\ncv = CrossValidation(fsclf,\n                     NFoldPartitioner(attr='subject'),\n                     errorfx=mean_match_accuracy)\nbsc_mni_results = cv(ds_mni)\nverbose(2, \"done in %.1f seconds\" % (time.time() - mni_start_time,))"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Between-subject classification with Hyperalignment(TM)"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "Between-subject classification using Hyperalignment is very similar\nto the previous analysis. However, now we no longer assume feature\ncorrespondence (or aren't satisfied with anatomical alignment anymore).\nConsequently, we have to transform the individual datasets into a common space\nbefore performing the classification analysis. To avoid introducing\ncircularity problems to the analysis, we perform leave-one-run-out\ndata folding manually, and train Hyperalignment only on the training\ndataset partitions. Subsequently, we will apply the derived transformation\nto the full datasets, stack them up across individual subjects, as before,\nand run the classification analysis. ANOVA-based feature selection is done\nin the same way as before (but also manually)."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "verbose(2, \"between-subject (hyperaligned)...\", cr=False, lf=False)\nhyper_start_time = time.time()\nbsc_hyper_results = []\n# same cross-validation over subjects as before\ncv = CrossValidation(clf, NFoldPartitioner(attr='subject'),\n                     errorfx=mean_match_accuracy)\n\n# leave-one-run-out for hyperalignment training\nfor test_run in range(nruns):\n    # split in training and testing set\n    ds_train = [sd[sd.sa.chunks != test_run,:] for sd in ds_all]\n    ds_test = [sd[sd.sa.chunks == test_run,:] for sd in ds_all]\n\n    # manual feature selection for every individual dataset in the list\n    anova = OneWayAnova()\n    fscores = [anova(sd) for sd in ds_train]\n    featsels = [StaticFeatureSelection(fselector(fscore)) for fscore in fscores]\n    ds_train_fs = [featsels[i].forward(sd) for i, sd in enumerate(ds_train)]\n\n\n    # Perform hyperalignment on the training data with default parameters.\n    # Computing hyperalignment parameters is as simple as calling the\n    # hyperalignment object with a list of datasets. All datasets must have the\n    # same number of samples and time-locked responses are assumed.\n    # Hyperalignment returns a list of mappers corresponding to subjects in the\n    # same order as the list of datasets we passed in.\n\n\n    hyper = Hyperalignment()\n    hypmaps = hyper(ds_train_fs)\n\n    # Applying hyperalignment parameters is similar to applying any mapper in\n    # PyMVPA. We start by selecting the voxels that we used to derive the\n    # hyperalignment parameters. And then apply the hyperalignment parameters\n    # by running the test dataset through the forward() function of the mapper.\n\n    ds_test_fs = [featsels[i].forward(sd) for i, sd in enumerate(ds_test)]\n    ds_hyper = [ hypmaps[i].forward(sd) for i, sd in enumerate(ds_test_fs)]\n\n    # Now, we have a list of datasets with feature correspondence in a common\n    # space derived from the training data. Just as in the between-subject\n    # analyses of anatomically aligned data we can stack them all up and run the\n    # crossvalidation analysis.\n\n    ds_hyper = vstack(ds_hyper)\n    # zscore each subject individually after transformation for optimal\n    # performance\n    zscore(ds_hyper, chunks_attr='subject')\n    res_cv = cv(ds_hyper)\n    bsc_hyper_results.append(res_cv)\n\nbsc_hyper_results = hstack(bsc_hyper_results)\nverbose(2, \"done in %.1f seconds\" % (time.time() - hyper_start_time,))"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Comparing the results"
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 3, 
          "metadata": {}, 
          "source": [
            "Performance"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "First we take a look at the classification performance (or accuracy) of all\nthree analysis approaches."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "verbose(1, \"Average classification accuracies:\")\nverbose(2, \"within-subject: %.2f +/-%.3f\"\n        % (np.mean(wsc_results),\n           np.std(wsc_results) / np.sqrt(nsubjs - 1)))\nverbose(2, \"between-subject (anatomically aligned): %.2f +/-%.3f\"\n        % (np.mean(bsc_mni_results),\n           np.std(np.mean(bsc_mni_results, axis=1)) / np.sqrt(nsubjs - 1)))\nverbose(2, \"between-subject (hyperaligned): %.2f +/-%.3f\" \\\n        % (np.mean(bsc_hyper_results),\n           np.std(np.mean(bsc_hyper_results, axis=1)) / np.sqrt(nsubjs - 1)))"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "The output of this demo looks like this:"
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "Loading data...\n 10 subjects\n Per-subject dataset: 56 samples with 3509 features\n Stimulus categories: Chair, DogFace, FemaleFace, House, MaleFace, MonkeyFace, Shoe\nPerforming classification analyses...\n within-subject... done in 4.3 seconds\n between-subject (anatomically aligned)...done after 3.2 seconds\n between-subject (hyperaligned)...done in 10.5 seconds\nAverage classification accuracies:\n within-subject: 0.57 +/-0.063\n between-subject (anatomically aligned): 0.42 +/-0.035\n between-subject (hyperaligned): 0.62 +/-0.046"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "It is obvious that the between-subject classification using anatomically\naligned data has significantly worse performance when compared to\nwithin-subject classification. Clearly the group classification model is\ninferior to individual classifiers fitted to a particular subject's data.\nHowever, a group classifier trained on hyperaligned data is performing at least\nas good as the within-subject classifiers -- possibly even slightly better due\nto the increased size of the training dataset."
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 3, 
          "metadata": {}, 
          "source": [
            "Similarity structures"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "To get a better understanding of how hyperalignment transforms the structure\nof the data, we compare the similarity structures of the corresponding input\ndatasets of all three analysis above (and one in addition).\n\n", 
            "These are respectively:", 
            "\n\n1. ", 
            "Average similarity structure of the individual data.", 
            "\n\n2. ", 
            "Similarity structure of the averaged hyperaligned data.", 
            "\n\n3. ", 
            "Average similarity structure of the individual data after hyperalignment.", 
            "\n\n4. ", 
            "Similarity structure of the averaged anatomically-aligned data.", 
            "\n\n", 
            "Similarity structure in this case is the correlation matrix of multivariate\nresponse patterns for all seven stimulus categories in the datasets. For\nthe sake of simplicity, all similarity structures are computed on the full\ndataset without data folding."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "# feature selection as above\nanova = OneWayAnova()\nfscores = [anova(sd) for sd in ds_all]\nfscores = np.mean(np.asarray(vstack(fscores)), axis=0)\n# apply to full datasets\nds_fs = [sd[:,fselector(fscores)] for i,sd in enumerate(ds_all)]\n#run hyperalignment on full datasets\nhyper = Hyperalignment()\nmappers = hyper(ds_fs)\nds_hyper = [ mappers[i].forward(ds_) for i,ds_ in enumerate(ds_fs)]\n# similarity of original data samples\nsm_orig = [np.corrcoef(\n                sd.get_mapped(\n                    mean_group_sample(['targets'])).samples)\n                        for sd in ds_fs]\n# mean across subjects\nsm_orig_mean = np.mean(sm_orig, axis=0)\n# same individual average but this time for hyperaligned data\nsm_hyper_mean = np.mean(\n                    [np.corrcoef(\n                        sd.get_mapped(mean_group_sample(['targets'])).samples)\n                            for sd in ds_hyper], axis=0)\n# similarity for averaged hyperaligned data\nds_hyper = vstack(ds_hyper)\nsm_hyper = np.corrcoef(ds_hyper.get_mapped(mean_group_sample(['targets'])))\n# similarity for averaged anatomically aligned data\nds_fs = vstack(ds_fs)\nsm_anat = np.corrcoef(ds_fs.get_mapped(mean_group_sample(['targets'])))"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "We then plot the respective similarity strucures."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "# class labels should be in more meaningful order for visualization\n# (human faces, animals faces, objects)\nintended_label_order = [2,4,1,5,3,0,6]\nlabels = ds_all[0].UT\nlabels = labels[intended_label_order]\n\npl.figure(figsize=(6,6))\n# plot all three similarity structures\nfor i, sm_t in enumerate((\n    (sm_orig_mean, \"Average within-subject\\nsimilarity\"),\n    (sm_anat, \"Similarity of group average\\ndata (anatomically aligned)\"),\n    (sm_hyper_mean, \"Average within-subject\\nsimilarity (hyperaligned data)\"),\n    (sm_hyper, \"Similarity of group average\\ndata (hyperaligned)\"),\n                      )):\n    sm, title = sm_t\n    # reorder matrix columns to match label order\n    sm = sm[intended_label_order][:,intended_label_order]\n    pl.subplot(2, 2, i+1)\n    pl.imshow(sm, vmin=-1.0, vmax=1.0, interpolation='nearest')\n    pl.colorbar(shrink=.4, ticks=[-1,0,1])\n    pl.title(title, size=12)\n    ylim = pl.ylim()\n    pl.xticks(range(ncats), labels, size='small', stretch='ultra-condensed',\n              rotation=45)\n    pl.yticks(range(ncats), labels, size='small', stretch='ultra-condensed',\n              rotation=45)\n    pl.ylim(ylim)"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n\\[Visit [http://pymvpa.org/examples/hyperalignment.html](http://pymvpa.org/examples/hyperalignment.html) to view this figure\\]\n\n", 
            "We can clearly see that averaging anatomically aligned data has a negative\neffect on the similarity structure, as the fine category structure is diminished\nand only the coarse structure (faces vs. objects) is preserved. Moreover, we can\nsee that after hyperalignment the average similarity structure of individual\ndata is essentially identical to the similarity structure of averaged data --\nreflecting the feature correspondence in the common high-dimensional space."
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Regularized Hyperalignment"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "According to ", 
            "*Xu et al. 2012*, Hyperalignment can be\nreformulated to a regularized algorithm that can span the whole continuum\nbetween ", 
            "[canonical correlation analysis (CCA)](http://en.wikipedia.org/wiki/Canonical_correlation) and regular hyperalignment by\nvarying a regularization parameter (alpha).  Here, we repeat the above\nbetween-subject hyperalignment and classification analyses with varying values\nof alpha from 0 (CCA) to 1.0 (regular hyperalignment).\n\n", 
            "The following code is essentially identical to the implementation of\nbetween-subject classification shown above. The only difference is an addition\n`for` loop doing the alpha value sweep for each cross-validation fold."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "alpha_levels = np.concatenate(\n                    (np.linspace(0.0, 0.7, 8),\n                     np.linspace(0.8, 1.0, 9)))\n# to collect the results for later visualization\nbsc_hyper_results = np.zeros((nsubjs, len(alpha_levels), nruns))\n# same cross-validation over subjects as before\ncv = CrossValidation(clf, NFoldPartitioner(attr='subject'),\n                     errorfx=mean_match_accuracy)\n\n# leave-one-run-out for hyperalignment training\nfor test_run in range(nruns):\n    # split in training and testing set\n    ds_train = [sd[sd.sa.chunks != test_run,:] for sd in ds_all]\n    ds_test = [sd[sd.sa.chunks == test_run,:] for sd in ds_all]\n\n    # manual feature selection for every individual dataset in the list\n    anova = OneWayAnova()\n    fscores = [anova(sd) for sd in ds_train]\n    featsels = [StaticFeatureSelection(fselector(fscore)) for fscore in fscores]\n    ds_train_fs = [featsels[i].forward(sd) for i, sd in enumerate(ds_train)]\n\n    for alpha_level, alpha in enumerate(alpha_levels):\n        hyper = Hyperalignment(alignment=ProcrusteanMapper(svd='dgesvd',\n                                                           space='commonspace'),\n                               alpha=alpha)\n        hypmaps = hyper(ds_train_fs)\n        ds_test_fs = [featsels[i].forward(sd) for i, sd in enumerate(ds_test)]\n        ds_hyper = [ hypmaps[i].forward(sd) for i, sd in enumerate(ds_test_fs)]\n        ds_hyper = vstack(ds_hyper)\n        zscore(ds_hyper, chunks_attr='subject')\n        res_cv = cv(ds_hyper)\n        bsc_hyper_results[:, alpha_level, test_run] = res_cv.samples.T"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "Now we can plot the classification accuracy as a function of regularization\nintensity."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "bsc_hyper_results = np.mean(bsc_hyper_results, axis=2)\npl.figure()\nplot_err_line(bsc_hyper_results, alpha_levels)\npl.xlabel('Regularization parameter: alpha')\npl.ylabel('Average BSC using hyperalignment +/- SEM')\npl.title('Using regularized hyperalignment with varying alpha values')"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n\\[Visit [http://pymvpa.org/examples/hyperalignment.html](http://pymvpa.org/examples/hyperalignment.html) to view this figure\\]\n\n", 
            "We can clearly see that the regular hyperalignment performs best for this\ndataset. However, please refer to ", 
            "*Xu et al. 2012* for an\nexample showing that this is not always the case."
          ]
        }
      ], 
      "metadata": {}
    }
  ]
}