{
  "metadata": {
    "name": "Analysis of eye movement patterns"
  }, 
  "nbformat": 3, 
  "nbformat_minor": 0, 
  "worksheets": [
    {
      "cells": [
        {
          "cell_type": "heading", 
          "level": 1, 
          "metadata": {}, 
          "source": [
            "Analysis of eye movement patterns"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "In this example we are going to look at a classification analysis of eye\nmovement patterns. Although complex preprocessing steps can be performed to\nextract higher-order features from the raw coordinate timeseries provided by an\neye-tracker, we are keeping it simple.\n\n", 
            "Right after importing the PyMVPA suite, we load the data from a textfile.\nIt contains coordinate timeseries of 144 trials (recorded with 350 Hz), where\nsubjects either looked at upright or inverted images of human faces. Each\ntimeseries snippet covers 3 seconds. This data has been pre-processed to remove\neyeblink artefacts.\n\n", 
            "In addition to the coordinates we also load trial attributes from a second\ntextfile. These attributes indicate which image was shown, whether it was\nshowing a male or female face, and wether it was upright or inverted."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "from mvpa2.suite import *\n\n# where is the data\ndatapath = os.path.join(pymvpa_datadbroot,\n                        'face_inversion_demo', 'face_inversion_demo')\n# (X, Y, trial id) for all timepoints\ndata = np.loadtxt(os.path.join(datapath, 'gaze_coords.txt'))\n# (orientation, gender, image id) for each trial\nattribs = np.loadtxt(os.path.join(datapath, 'trial_attrs.txt'))"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "As a first step we put the coordinate timeseries into a dataset, and labels each\ntimepoint with its associated trial ID. We also label the two features\naccordingly."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "raw_ds = Dataset(data[:,:2],\n                 sa = {'trial': data[:,2]},\n                 fa = {'fid': ['rawX', 'rawY']})"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "The second step is down-sampling the data to about 33 Hz, resampling each trial\ntimeseries individually (using the trial ID attribute to define dataset chunks)."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "ds = fft_resample(raw_ds, 100, window='hann',\n                  chunks_attr='trial', attr_strategy='sample')"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "Now we can use a ", 
            "[BoxcarMapper](http://pymvpa.org/generated/mvpa2.mappers.boxcar.BoxcarMapper.html#mvpa2-mappers-boxcar-boxcarmapper) to turn each\ntrial-timeseries into an individual sample. We know that each sample consists\nof 100 timepoints. After the dataset is mapped we can add all per-trial\nattributes into the sample attribute collection."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "bm = BoxcarMapper(np.arange(len(ds.sa['trial'].unique)) * 100,\n                  boxlength=100)\nbm.train(ds)\nds=ds.get_mapped(bm)\n\nds.sa.update({'orient': attribs[:,0].astype(int),\n              'gender': attribs[:,1].astype(int),\n              'img_id': attribs[:,1].astype(int)})"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "In comparison with upright faces, inverted ones had prominent features at very\ndifferent locations on the screen. Most notably, the eyes were flipped to the\nbottom half. To prevent the classifier from using such differences, we flip the\nY-coordinates for trials with inverted to align the with the upright condition."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "ds.samples[ds.sa.orient == 1, :, 1] = \\\n        -1 * (ds.samples[ds.sa.orient == 1, :, 1] - 512) + 512"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "The current dataset has 100 two-dimensional features, the X and Y\ncoordinate for each of the hundred timepoints. We use a\n", 
            "[FlattenMapper](http://pymvpa.org/generated/mvpa2.mappers.flatten.FlattenMapper.html#mvpa2-mappers-flatten-flattenmapper) to convert each sample into a\none-dimensionl vector (of length 200). However, we also keep the original\ndataset, because it will allow us to perform some plotting much easier."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "fm = FlattenMapper()\nfm.train(ds)\n# want to make a copy to keep the original pristine for later plotting\nfds = ds.copy().get_mapped(fm)\n\n# simplify the trial attribute\nfds.sa['trial'] = [t[0] for t in ds.sa.trial]"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "The last steps of preprocessing are Z-scoring all features\n(coordinate-timepoints) and dividing the dataset into 8 chunks -- to simplify\na cross-validation analysis."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "zscore(fds, chunks_attr=None)\n\n# for classification divide the data into chunks\nnchunks =  8\nchunks = np.zeros(len(fds), dtype='int')\nfor o in fds.sa['orient'].unique:\n    chunks[fds.sa.orient == o] = np.arange(len(fds.sa.orient == o)) % nchunks\nfds.sa['chunks'] = chunks"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "Now everything is set and we can proceed to the classification analysis. We\nare using a support vector machine that is going to be trained on the\n`orient` attribute, indicating trials with upright and inverted faces. We are\ngoing to perform the analysis with a ", 
            "[SplitClassifier](http://pymvpa.org/generated/mvpa2.clfs.meta.SplitClassifier.html#mvpa2-clfs-meta-splitclassifier),\nbecause we are also interested in the temporal sensitivity profile. That one is\neasily accessible via the corresponding sensitivity analyzer."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "clf = SVM(space='orient')\nmclf = SplitClassifier(clf, space='orient',\n                       enable_ca=['confusion'])\nsensana = mclf.get_sensitivity_analyzer()\nsens = sensana(fds)\nprint mclf.ca.confusion"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "The 8-fold cross-validation shows a trial-wise classification accuracy of\nover 80%. Now we can take a look at the sensitivity. We use the\n", 
            "[FlattenMapper](http://pymvpa.org/generated/mvpa2.mappers.flatten.FlattenMapper.html#mvpa2-mappers-flatten-flattenmapper) that is stored in the dataset to\nunmangle X and Y coordinate vectors in the sensitivity array."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "# split mean sensitivities into X and Y coordinate parts by reversing through\n# the flatten mapper\nxy_sens = fds.a.mapper[-2].reverse(sens).samples"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Plotting the results"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "The analysis is done and we can compile a figure to visualize the results.\nAfter some inital preparations, we plot an example image of a face that was\nused in this experiment. We align the image coordinates with the original\non-screen coordinates to match them to the gaze track, and overlay the image\nwith the mean gaze track across all trials for each condition."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "# descriptive plots\npl.figure()\n# original screen size was\naxes = ('x', 'y')\nscreen_size = np.array((1280, 1024))\nscreen_center = screen_size / 2\ncolors = ('r','b')\nfig = 1\n\npl.subplot(2, 2, fig)\npl.title('Mean Gaze Track')\nface_img = pl.imread(os.path.join(datapath, 'demo_face.png'))\n# determine the extend of the image in original screen coordinates\n# to match with gaze position\norig_img_extent=(screen_center[0] - face_img.shape[1]/2,\n                 screen_center[0] + face_img.shape[1]/2,\n                 screen_center[1] + face_img.shape[0]/2,\n                 screen_center[1] - face_img.shape[0]/2)\n# show face image and put it with original pixel coordinates\npl.imshow(face_img,\n          extent=orig_img_extent,\n          cmap=pl.cm.gray)\npl.plot(np.mean(ds.samples[ds.sa.orient == 1,:,0], axis=0),\n        np.mean(ds.samples[ds.sa.orient == 1,:,1], axis=0),\n        colors[0], label='inverted')\npl.plot(np.mean(ds.samples[ds.sa.orient == 2,:,0], axis=0),\n        np.mean(ds.samples[ds.sa.orient == 2,:,1], axis=0),\n        colors[1], label='upright')\npl.axis(orig_img_extent)\npl.legend()\nfig += 1"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "The next two subplot contain the gaze coordinate over the peri-stimulus time\nfor both, X and Y axis respectively."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "pl.subplot(2, 2, fig)\npl.title('Gaze Position X-Coordinate')\nplot_erp(ds.samples[ds.sa.orient == 1,:,1], pre=0, errtype = 'std',\n         color=colors[0], SR=100./3.)\nplot_erp(ds.samples[ds.sa.orient == 2,:,1], pre=0, errtype = 'std',\n         color=colors[1], SR=100./3.)\npl.ylim(orig_img_extent[2:])\npl.xlabel('Peristimulus Time')\nfig += 1\n\npl.subplot(2, 2, fig)\npl.title('Gaze Position Y-Coordinate')\nplot_erp(ds.samples[ds.sa.orient == 1,:,0], pre=0, errtype = 'std',\n         color=colors[0], SR=100./3.)\nplot_erp(ds.samples[ds.sa.orient == 2,:,0], pre=0, errtype = 'std',\n         color=colors[1], SR=100./3.)\npl.ylim(orig_img_extent[:2])\npl.xlabel('Peristimulus Time')\nfig += 1"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "The last panel has the associated sensitivity profile for both coordinate axes."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "pl.subplot(2, 2, fig)\npl.title('SVM-Sensitivity Profiles')\nlines = plot_err_line(xy_sens[..., 0], linestyle='-', fmt='ko', errtype='std')\nlines[0][0].set_label('X')\nlines = plot_err_line(xy_sens[..., 1], linestyle='-', fmt='go', errtype='std')\nlines[0][0].set_label('Y')\npl.legend()\npl.ylim((-0.1, 0.1))\npl.xlim(0,100)\npl.axhline(y=0, color='0.6', ls='--')\npl.xlabel('Timepoints')\n\nfrom mvpa2.base import cfg"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "The following figure is not exactly identical to the product of this code, but\nrather shows the result of a few minutes of beautifications in ", 
            "[Inkscape](http://www.inkscape.org/).\n\n\\[Visit [http://pymvpa.org/examples/eyemovements.html](http://pymvpa.org/examples/eyemovements.html) to view this figure\\]"
          ]
        }
      ], 
      "metadata": {}
    }
  ]
}