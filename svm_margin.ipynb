{
  "metadata": {
    "name": "Analysis of the margin width in a soft-margin SVM"
  }, 
  "nbformat": 3, 
  "nbformat_minor": 0, 
  "worksheets": [
    {
      "cells": [
        {
          "cell_type": "heading", 
          "level": 1, 
          "metadata": {}, 
          "source": [
            "Analysis of the margin width in a soft-margin SVM"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "Width of the margin of soft-margin SVM\n(", 
            "[None](http://pymvpa.org/generated/mvpa2.clfs.svm.LinearCSVMC.html#mvpa2-clfs-svm-linearcsvmc)) is not monotonic in its relation\nwith SNR of the data.  In case of not perfectly separable classes\nmargin would first shrink with the increase of SNR, and then start to\nexpand again after learning error becomes sufficiently small.\n\n", 
            "This brief examples provides a demonstration."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "import mvpa2\nimport pylab as pl\nimport numpy as np\nfrom mvpa2.misc.data_generators import normal_feature_dataset\nfrom mvpa2.clfs.svm import LinearCSVMC\nfrom mvpa2.generators.partition import NFoldPartitioner\nfrom mvpa2.measures.base import CrossValidation\nfrom mvpa2.mappers.zscore import zscore"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "Generate a binary dataset without any signal (snr=0)."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "mvpa2.seed(1);\nds_noise = normal_feature_dataset(perlabel=100, nlabels=2, nfeatures=2, snr=0,\n                                  nonbogus_features=[0,1])\n\n# signal levels\nsigs = [0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "To mimic behavior of hard-margin SVM whenever classes become\nseparable, which is easier to comprehend, we are intentionally setting\nvery high C value."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "clf = LinearCSVMC(C=1000, enable_ca=['training_stats'])\ncve = CrossValidation(clf, NFoldPartitioner(), enable_ca='stats')\nsana = clf.get_sensitivity_analyzer(postproc=None)\n\nrs = []\nerrors, training_errors = [], []\n\nfor sig in sigs:\n    ds = ds_noise.copy()\n    # introduce signal into the first feature\n    ds.samples[ds.T == 'L1', 0] += sig\n\n    error = np.mean(cve(ds))\n    sa = sana(ds)\n    training_error = 1-clf.ca.training_stats.stats['ACC']\n\n    errors.append(error)\n    training_errors.append(training_error)\n\n    w = sa.samples[0]\n    b = np.asscalar(sa.sa.biases)\n    # width each way\n    r = 1./np.linalg.norm(w)\n\n    msg = \"SIGNAL: %.2f training_error: %.2f error: %.2f |w|: %.2f r=%.2f\" \\\n      %(sig, training_error, error, np.linalg.norm(w), r)\n    print msg\n\n    # Drawing current data and SVM hyperplane+margin\n    xmin = np.min(ds[:,0], axis=0)\n    xmax = np.max(ds[:,0], axis=0)\n    x = np.linspace(xmin, xmax, 20)\n    y  =    -(w[0] * x - b) /w[1]\n    y1 = ( 1-(w[0] * x - b))/w[1]\n    y2 = (-1-(w[0] * x - b))/w[1]\n\n    pl.figure(figsize=(10,4))\n\n    for t,c in zip(ds.UT, ['r', 'b']):\n        ds_ = ds[ds.T == t]\n        pl.scatter(ds_[:, 0], ds_[:, 1], c=c)\n    # draw the hyperplane\n    pl.plot(x, y)\n    pl.plot(x, y1, '--')\n    pl.plot(x, y2, '--')\n    pl.title(msg)\n    ca = pl.gca()\n    ca.set_xlim((-2, 4))\n    ca.set_ylim((-1.2, 1.2))\n    pl.show()\n    rs.append(r)"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "So what would be our dependence between signal level and errors/width\nof the margin?"
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "pl.figure()\npl.plot(sigs, rs, label=\"Margin width of %s\" % clf)\npl.plot(sigs, errors, label=\"CV error\")\npl.plot(sigs, training_errors, label=\"Training error\")\npl.xlabel(\"Signal\")\npl.legend()\npl.show()"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "And this is how it looks like.\n\n\\[Visit [http://pymvpa.org/examples/svm_margin.html](http://pymvpa.org/examples/svm_margin.html) to view this figure\\]"
          ]
        }
      ], 
      "metadata": {}
    }
  ]
}