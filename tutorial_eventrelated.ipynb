{
  "metadata": {
    "name": "Event-related Data Analysis"
  }, 
  "nbformat": 3, 
  "nbformat_minor": 0, 
  "worksheets": [
    {
      "cells": [
        {
          "cell_type": "heading", 
          "level": 1, 
          "metadata": {}, 
          "source": [
            "Event-related Data Analysis"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "In all previous tutorial parts we have analyzed the same fMRI data. We analyzed\nit using a number of different strategies, but they all had one thing in\ncommon: A sample in each dataset was always a single volume from an fMRI time\nseries.  Sometimes, we have limited ourselves to just a specific temporal\nwindows of interest, sometimes we averaged many fMRI volumes into a single one.\nIn all cases, however, a feature always corresponded to a voxel in the fMRI\nvolume and appeared only once in the dataset.\n\n", 
            "In this part we are going to extend the analysis beyond the spatial\ndimensions and will consider ", 
            "*time* as another aspect of our data.\nWe will demonstrate two different approaches: 1) modeling of experimental\nconditions and proceed with an analysis of model parameter estimates, and\n2) the extraction of spatio-temporal data samples. The latter approach is\ncommon, for example, in ERP-analyses of EEG data.\n\n", 
            "Let's start with our well-known example dataset -- this time selecting a subset\nof ventral temporal regions."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "from mvpa2.tutorial_suite import *\n", 
            "ds = get_raw_haxby2001_data(roi=(36,38,39,40))"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "As we know, this dataset consists of 12 concatenated experiment sessions.\nEvery session had a stimulation block spanning multiple fMRI volumes for\neach of the eight stimulus categories. Stimulation blocks were separated by\nrest periods."
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Event-related Pre-processing Is Not Event-related"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "For an event-related analysis, most of the processing is done on data\nsamples that are somehow derived from a set of events. The rest of the data\ncould be considered irrelevant. However, some preprocessing is only\nmeaningful when performed on the full time series and not on the segmented\nevent samples. An example is the detrending that typically needs to be done\non the original, continuous time series.\n\n", 
            "In its current shape our datasets consists of samples that represent contiguous\nfMRI volumes. At this stage we can easily perform linear detrending."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "poly_detrend(ds, polyord=1, chunks_attr='chunks')"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "Let's make a copy of the de-trended dataset that we can use later on for\nsome visualization."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "orig_ds = ds.copy()"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Design Specification"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "For any event-related analysis we need some information on the experiment\ndesign: when was stimulated with what for how long (and maybe with what\nintensity).  In PyMVPA this is done by compiling a list of event definitions.\nIn many cases, an event is defined by ", 
            "*onset*, ", 
            "*duration* and potentially a\nnumber of additional properties, such as stimulus condition or recording\nsession number.\n\n", 
            "To see how such events definitions look like, we will simply convert the\nblock-design setup defined by the samples attributes of our dataset into a list\nof events.  With ", 
            "[find_events()](http://pymvpa.org/generated/mvpa2.datasets.eventrelated.find_events.html#mvpa2-datasets-eventrelated-find-events), PyMVPA\nprovides a function to convert sequential attributes into event lists. In our\ndataset, we have the stimulus conditions of each volume sample available in the\n`targets` sample attribute."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "events = find_events(targets=ds.sa.targets, chunks=ds.sa.chunks)\n", 
            "print len(events)"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "for e in events[:4]:\n   print e"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "We are feeding not only the `targets` to the function, but also the\n`chunks` attribute, since we do not want to have events spanning multiple\nrecording sessions. ", 
            "[find_events()](http://pymvpa.org/generated/mvpa2.datasets.eventrelated.find_events.html#mvpa2-datasets-eventrelated-find-events)\nsequentially parses all provided attributes and records an event whenever the\nvalue in ", 
            "*any* of the attributes changes. The generated event definition is a\ndictionary that contains:", 
            "\n\n1. ", 
            "Onset of the event as an index in the sequence (in this example this is a\nvolume id)", 
            "\n\n2. ", 
            "Duration of the event in \"number of sequence elements\" (i.e. number of\nvolumes). The duration is determined by counting the number of identical\nattribute combinations following an event onset.", 
            "\n\n3. ", 
            "Attribute combination of this event, i.e. the actual values of all given\nattributes at the particular position.", 
            "\n\n", 
            "Let's limit ourselves to `face` and `house` stimulation blocks for now.\nWe can easily filter out all other events."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "events = [ev for ev in events if ev['targets'] in ['house', 'face']]\n", 
            "print len(events)"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "for e in events[:4]:\n   print e"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Response Modeling"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "Whenever we have to deal with data were multiple concurrent signals\nare overlapping in time, such as in fast event-related fMRI studies,\nit often makes sense to fit an appropriate model to the data and\nproceed with an analysis of model parameter estimates, instead of\nthe raw data.\n\n", 
            "PyMVPA can make use of NiPy's GLM modeling capabilities. It expects\ninformation on stimulation events to be given as actual time stamps\nand not data sample indices, hence we have to convert our event list."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "TR = np.median(np.diff(ds.sa.time_coords))\n", 
            "for ev in events:\n    ev['onset'] = (ev['onset'] * TR)\n    ev['duration'] = ev['duration'] * TR"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "Now we can fit a model of the hemodynamic response to all relevant\nstimulus conditions. The function\n", 
            "[eventrelated_dataset()](http://pymvpa.org/generated/mvpa2.datasets.eventrelated.eventrelated_dataset.html#mvpa2-datasets-eventrelated-eventrelated-dataset) does everything\nfor us. For a given input dataset we need to provide a list of events,\nthe name of an attribute with a time stamp for each sample, and information\non what conditions we would like to have modeled. The latter is specified\nto the `condition_attr` argument. This can be a single attribute name\nin which case all unique values will be used as conditions. It can also\nbe a sequence of multiple attribute names, and all combinations of unique\nvalues of the attributes will be used as conditions. In the following example\n`('targets', 'chunks')` indicates that we want a separate model for each\nstimulation condition (`targets`) for each run of our example dataset\n(`chunks`)."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "evds = fit_event_hrf_model(ds,\n                           events,\n                           time_attr='time_coords',\n                           condition_attr=('targets', 'chunks'))\n", 
            "print len(evds)"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "This yields one parameter estimate sample for each target value for each\nchunks."
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "- - -\n**Exercise**", 
            "\n\n", 
            "Explore the `evds` dataset. It contains the generated HRF model.\nFind and plot (some of) them. Take a look at the parameter estimate\nsamples themselves -- can you spot a pattern?"
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "# you can use this cell for this exercise"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "- - -\n", 
            "\n\n", 
            "Before we can run a classification analysis we still need to normalize each\nfeature (GLM parameters estimates for each voxel at this point)."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "zscore(evds, chunks_attr=None)"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "The rest is straight-forward: we set up a cross-validation analysis with\na chosen classifier and run it:"
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "clf = kNN(k=1, dfx=one_minus_correlation, voting='majority')\n", 
            "cv = CrossValidation(clf, NFoldPartitioner(attr='chunks'))\n", 
            "cv_glm = cv(evds)\n", 
            "print '%.2f' % np.mean(cv_glm)"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "Not bad! Let's compare that to a simpler approach that is also suitable for\nblock-design experiments like this one."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "zscore(ds, param_est=('targets', ['rest']))\n", 
            "avgds = ds.get_mapped(mean_group_sample(['targets', 'chunks']))\n", 
            "avgds = avgds[np.array([t in ['face', 'house'] for t in avgds.sa.targets])]"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "We normalize all voxels with respect to the `rest` condition. This yields\nsome crude kind of \"activation\" score for all stimulation conditions.\nSubsequently, we average all sample of a condition in each run. This yield\na dataset of the same size as from the GLM modeling. We can re-use the\ncross-validation setup."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "cv_avg = cv(avgds)\n", 
            "print '%.2f' % np.mean(cv_avg)"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "Not bad either. However, it is worth repeating that this simple average-sample\napproach is limited to block-designs with a clear temporal separation of\nall signals of interest, whereas the HRF modeling is more suitable for\nexperiments with fast stimulation alternation."
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "- - -\n**Exercise**", 
            "\n\n", 
            "Think about what need to be done to perform odd/even run GLM modeling."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "# you can use this cell for this exercise"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "- - -\n"
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "From Timeseries To Spatio-temporal Samples"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "Now we want to try something different. Instead of compressing all temporal\ninformation into a single model parameter estimate, we can also consider the\nentire spatio-temporal signal across our region of interest and the full\nduration of the stimulation blocks. In other words, we can perform a\nsensitivity analysis (see ", 
            "*chap_tutorial_sensitivity*) revealing the\nspatio-temporal distribution of classification-relevant information.\n\n", 
            "Before we start with our event-extraction, we want to normalize each feature\n(i.e. a voxel at this point). In this case we are, again, going to Z-score\nthem, using the mean and standard deviation from the experiment's rest\ncondition, and the resulting values might be interpreted as \"activation\nscores\"."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "zscore(ds, chunks_attr='chunks', param_est=('targets', 'rest'))"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "For this analysis we do not have to convert event onset information into\ntime-stamp, but can operate on sample indices, hence we start with the\noriginal event list again."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "events = find_events(targets=ds.sa.targets, chunks=ds.sa.chunks)\n", 
            "events = [ev for ev in events if ev['targets'] in ['house', 'face']]"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "All of our events are of the same length, 9 consecutive fMRI volumes. Later\non we would like to view the temporal sensitivity profile from ", 
            "*before* until\n", 
            "*after* the stimulation block, hence we should extend the duration of the\nevents a bit."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "event_duration = 13\n", 
            "for ev in events:\n    ev['onset'] -= 2\n    ev['duration'] = event_duration"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "The next and most important step is to actually segment the original\ntime series dataset into event-related samples. PyMVPA offers\n", 
            "[eventrelated_dataset()](http://pymvpa.org/generated/mvpa2.datasets.eventrelated.eventrelated_dataset.html#mvpa2-datasets-eventrelated-eventrelated-dataset) as a function to\nperform this conversion. Let's just do it, it only needs the original\ndataset and our list of events."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "evds = eventrelated_dataset(ds, events=events)\n", 
            "len(evds) == len(events)"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "evds.nfeatures == ds.nfeatures * event_duration"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "- - -\n**Exercise**", 
            "\n\n", 
            "Inspect the `evds` dataset. It has a fairly large number of attributes\n-- both for samples and for features. Look at each of them and think\nabout what it could be useful for."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "# you can use this cell for this exercise"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "- - -\n", 
            "\n\n", 
            "At this point it is worth looking at the dataset's mapper -- in particular at\nthe last two items in the chain mapper that have been added during the\nconversion into events."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "print evds.a.mapper[-2:]"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "- - -\n**Exercise**", 
            "\n\n", 
            "Reverse-map a single sample through the last two items in the chain\nmapper. Inspect the result and make sure it doesn't surprise. Now,\nreverse-map multiple samples at once and compare the result. Is this what\nyou would expect?"
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "# you can use this cell for this exercise"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "- - -\n", 
            "\n\n", 
            "The rest of our analysis is business as usual and is quickly done.  We want to\nperform a cross-validation analysis of an SVM classifier. We are not\nprimarily interested in its performance, but in the weights it assigns to\nthe features. Remember, each feature is now voxel-at-time-point, so we get a\nchance of looking at the spatio-temporal profile of classification-relevant\ninformation in the data. We will nevertheless enable computing of a confusion\nmatrix, so we can assure ourselves that the classifier is performing\nreasonably well, because only a generalizing model is worth\ninspecting, as otherwise it overfits and the assigned weights\ncould be meaningless."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "sclf = SplitClassifier(LinearCSVMC(),\n                       enable_ca=['stats'])\n", 
            "sensana = sclf.get_sensitivity_analyzer()\n", 
            "sens = sensana(evds)"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "- - -\n**Exercise**", 
            "\n\n", 
            "Check that the classifier achieves an acceptable accuracy. Is it\nenough above chance level to allow for an interpretation of the\nsensitivities?"
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "# you can use this cell for this exercise"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "- - -\n"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "- - -\n**Exercise**", 
            "\n\n", 
            "Using what you have learned in the last tutorial part: Combine the\nsensitivity maps for all splits into a single map. Project this map into\nthe original dataspace. What is the shape of that space? Store the\nprojected map into a NIfTI file and inspect it using an MRI viewer.\nViewer needs to be capable of visualizing time series (hint: for FSLView\nthe time series image has to be opened first)!"
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "# you can use this cell for this exercise"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "- - -\n"
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "A Plotting Example"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "We have inspected the spatio-temporal profile of the sensitivities using\nsome MRI viewer application, but we can also assemble an informative figure\nright here. Let's compose a figure that shows the original peri-stimulus\ntime series, the effect of normalization, as well as the corresponding\nsensitivity profile of the trained SVM classifier. We are going to do that\nfor two example voxels, whose coordinates we might have derived from\ninspecting the full map."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "example_voxels = [(28,25,25), (28,23,25)]"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "The plotting will be done by the popular ", 
            "[matplotlib](http://matplotlib.sourceforge.net/) package.\n\n", 
            "First, we plot the original signal after initial detrending. To do this, we\napply the same time series segmentation to the original detrended dataset\nand plot the mean signal for all face and house events for both of our\nexample voxels. The code below will create the plot using matplotlib's\n`pylab` interface (imported as `pl`). If you are familiar with Matlab's\nplotting facilities, this shouldn't be hard to read."
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "- - -\n*Note*", 
            "\n\n", 
            "`_ =` is used in the examples below simply to absorb output of plotting\nfunctions.  You do not have to swallow output in your interactive sessions.", 
            "- - -\n"
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "vx_lty = ['-', '--']\n", 
            "t_col = ['b', 'r']\n", 
            "for i, v in enumerate(example_voxels):\n    # get a slicing array matching just to current example voxel\n    slicer = np.array([tuple(idx) == v for idx in ds.fa.voxel_indices])\n    # perform the timeseries segmentation just for this voxel\n    evds_detrend = eventrelated_dataset(orig_ds[:, slicer], events=events)\n    # now plot the mean timeseries and standard error\n    for j, t in enumerate(evds.uniquetargets):\n        l = plot_err_line(evds_detrend[evds_detrend.sa.targets == t].samples,\n                          fmt=t_col[j], linestyle=vx_lty[i])\n        # label this plot for automatic legend generation\n        l[0][0].set_label('Voxel %i: %s' % (i, t))\n", 
            "_ = pl.ylabel('Detrended signal')\n", 
            "_ = pl.axhline(linestyle='--', color='0.6')\n", 
            "_ = pl.legend()\n", 
            "_ = pl.xlim((0,12))"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "In the next figure we do exactly the same again, but this time for the\nnormalized data."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "for i, v in enumerate(example_voxels):\n    slicer = np.array([tuple(idx) == v for idx in ds.fa.voxel_indices])\n    evds_norm = eventrelated_dataset(ds[:, slicer], events=events)\n    for j, t in enumerate(evds.uniquetargets):\n        l = plot_err_line(evds_norm[evds_norm.sa.targets == t].samples,\n                          fmt=t_col[j], linestyle=vx_lty[i])\n        l[0][0].set_label('Voxel %i: %s' % (i, t))\n", 
            "_ = pl.ylabel('Normalized signal')\n", 
            "_ = pl.axhline(linestyle='--', color='0.6')\n", 
            "_ = pl.xlim((0,12))"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "Finally, we plot the associated SVM weight profile for each peri-stimulus\ntime-point of both voxels. For easier selection we do a little trick and\nreverse-map the sensitivity profile through the last mapper in the\ndataset's chain mapper (look at `evds.a.mapper` for the whole chain).\nThis will reshape the sensitivities into `cross-validation fold x volume x\nvoxel features`."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "normed = sens.get_mapped(FxMapper(axis='features', fx=l1_normed))\n", 
            "smaps = evds.a.mapper[-1].reverse(normed)\n", 
            "for i, v in enumerate(example_voxels):\n    slicer = np.array([tuple(idx) == v for idx in ds.fa.voxel_indices])\n    smap = smaps.samples[:,:,slicer].squeeze()\n    l = plot_err_line(smap, fmt='ko', linestyle=vx_lty[i], errtype='std')\n", 
            "_ = pl.xlim((0,12))\n", 
            "_ = pl.ylabel('Sensitivity')\n", 
            "_ = pl.axhline(linestyle='--', color='0.6')\n", 
            "_ = pl.xlabel('Peristimulus volumes')"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n\n", 
            "That was it. Perhaps you are scared by the amount of code. Please note that\nit could have been done shorter, but this way allows for plotting any other voxel\ncoordinate combination as well. matplotlib also allows for saving this figure in\n", 
            "[SVG](http://en.wikipedia.org/wiki/Scalable_Vector_Graphics) format, allowing for convenient post-processing in ", 
            "[Inkscape](http://www.inkscape.org/) -- a\npublication quality figure is only minutes away.\n\n\\[Visit [http://pymvpa.org/examples/tutorial_eventrelated.html](http://pymvpa.org/examples/tutorial_eventrelated.html) to view this figure\\]"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "- - -\n**Exercise**", 
            "\n\n", 
            "What can we say about the properties of the example voxel's signal from\nthe peri-stimulus plot?"
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "# you can use this cell for this exercise"
          ], 
          "language": "python", 
          "metadata": {}, 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "- - -\n", 
            "\n\n", 
            "This demo showed an event-related data analysis. Although we have performed\nit on fMRI data, an analogous analysis can be done for any time series based\ndata in an almost identical fashion. Moreover, if a dataset has information\nabout acquisition time (e.g. like the ones created by\n", 
            "[fmri_dataset()](http://pymvpa.org/generated/mvpa2.datasets.mri.fmri_dataset.html#mvpa2-datasets-mri-fmri-dataset))\n", 
            "[eventrelated_dataset()](http://pymvpa.org/generated/mvpa2.datasets.eventrelated.eventrelated_dataset.html#mvpa2-datasets-eventrelated-eventrelated-dataset) can also convert\nevent-definition in real time, making it relatively easy to \"convert\"\nexperiment design logfiles into event lists. In this case there would be no\nneed to run a function like\n", 
            "[find_events()](http://pymvpa.org/generated/mvpa2.datasets.eventrelated.find_events.html#mvpa2-datasets-eventrelated-find-events), but instead they could be\ndirectly specified and passed to\n", 
            "[eventrelated_dataset()](http://pymvpa.org/generated/mvpa2.datasets.eventrelated.eventrelated_dataset.html#mvpa2-datasets-eventrelated-eventrelated-dataset)."
          ]
        }
      ], 
      "metadata": {}
    }
  ]
}